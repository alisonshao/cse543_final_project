{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d667a16",
   "metadata": {},
   "source": [
    "# Minimal DDPM Denoising Example\n",
    "\n",
    "This notebook shows how to train and test a Diffusion Model (DDPM) using [lucidrains’ denoising-diffusion-pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch) for image denoising.\n",
    "\n",
    "- We use a *fake* dataset (random images) just for a quick demo.\n",
    "- We do a very short training, then demonstrate *partial inference* to remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a27ee",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "!pip install denoising_diffusion_pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668d8c5",
   "metadata": {},
   "source": [
    "## 1. Create a tiny (fake) dataset\n",
    "We'll use `torchvision.datasets.FakeData` just for illustration.\n",
    "You can replace this with your *real* image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our images will be 3-channel (RGB-like), 64x64\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.FakeData(\n",
    "    size=256,  # just 256 random images for quick demo\n",
    "    image_size=(3, image_size, image_size),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9f764",
   "metadata": {},
   "source": [
    "## 2. Define U-Net + DDPM\n",
    "We set fewer timesteps (50) for faster experimentation. You’ll likely want ~1000 for real applications.\n",
    "\n",
    "We also define a small U-Net for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    dim=32,                 # base channel dimension (tiny for speed)\n",
    "    dim_mults=(1, 2),       # how it expands at deeper layers\n",
    "    channels=3              # RGB\n",
    ").to(device)\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=image_size,\n",
    "    timesteps=50,           # fewer steps for quick demo\n",
    "    sampling_timesteps=50,  # same as timesteps here\n",
    "    loss_type='l2'          # can be 'l1', 'l2', or 'huber'\n",
    ").to(device)\n",
    "\n",
    "# We won't use the built-in Trainer class here, so we can demonstrate the custom partial inference later.\n",
    "# But you could also do: trainer = Trainer(diffusion, train_data, ...)\n",
    "print('Model and diffusion created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea05c09",
   "metadata": {},
   "source": [
    "## 3. Short Training Loop\n",
    "We’ll do just a few hundred steps so we can finish quickly. *Real training would require many thousands of steps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(diffusion.model.parameters(), lr=1e-4)\n",
    "\n",
    "num_training_steps = 300  # Just 300 iterations!\n",
    "step = 0\n",
    "\n",
    "diffusion.train()\n",
    "while step < num_training_steps:\n",
    "    for batch, _ in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        loss = diffusion(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step} / {num_training_steps}, Loss: {loss.item():.4f}\")\n",
    "        if step >= num_training_steps:\n",
    "            break\n",
    "\n",
    "print('Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd8229",
   "metadata": {},
   "source": [
    "## 4. Demonstration of Partial Inference (Denoising)\n",
    "We’ll create a random image from our dataset, manually add noise to it, and pretend it corresponds to some middle diffusion step. Then we’ll call a simple *custom partial denoising function* to show how you might do real-world denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor_img, title=\"\"):\n",
    "    # tensor_img: (C, H, W)\n",
    "    img_np = tensor_img.permute(1,2,0).detach().cpu().numpy()\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_diffusion_pytorch.denoising_diffusion_pytorch import extract\n",
    "\n",
    "@torch.no_grad()\n",
    "def partial_denoise(diffusion_model, x_t, t_start):\n",
    "    \"\"\"\n",
    "    x_t: a noisy image at diffusion step t_start (Tensor, shape [B, C, H, W])\n",
    "    t_start: integer step where 0 <= t_start < timesteps\n",
    "    \"\"\"\n",
    "    model = diffusion_model.model\n",
    "    betas = diffusion_model.betas\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    posterior_variance = diffusion_model.posterior_variance\n",
    "\n",
    "    model.eval()\n",
    "    device = x_t.device\n",
    "    for i in reversed(range(t_start+1)):\n",
    "        t_tensor = torch.tensor([i], device=device, dtype=torch.long).expand(x_t.shape[0])\n",
    "        # Predict noise\n",
    "        pred_noise = model(x_t, t_tensor)\n",
    "\n",
    "        beta_t = extract(betas, t_tensor, x_t.shape)\n",
    "        alpha_t = 1. - beta_t\n",
    "        alpha_bar_t = extract(alphas_cumprod, t_tensor, x_t.shape)\n",
    "\n",
    "        # Estimate x_0 via the standard DDPM formula\n",
    "        sqrt_recip_alpha_t = 1. / torch.sqrt(alpha_t)\n",
    "        x_0_pred = sqrt_recip_alpha_t * x_t - (beta_t / torch.sqrt(1 - alpha_bar_t)) * pred_noise\n",
    "\n",
    "        if i > 0:\n",
    "            # Compute the mean of x_{t-1}\n",
    "            alpha_bar_t1 = extract(alphas_cumprod, t_tensor-1, x_t.shape)\n",
    "            mean_pred = ( torch.sqrt(alpha_bar_t1) * x_0_pred\n",
    "                         + torch.sqrt(1 - alpha_bar_t1) * pred_noise )\n",
    "\n",
    "            # Add random noise based on posterior variance for sampling\n",
    "            posterior_var_t = extract(posterior_variance, t_tensor, x_t.shape)\n",
    "            noise = torch.randn_like(x_t)\n",
    "            x_t = mean_pred + torch.sqrt(posterior_var_t) * noise\n",
    "        else:\n",
    "            # At step 0, no more noise needed\n",
    "            x_t = x_0_pred\n",
    "\n",
    "    return x_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5c082",
   "metadata": {},
   "source": [
    "### 4.1 Create a \"Noisy\" Image\n",
    "Pick a real (random) image from the dataset, artificially diffuse it to step ~25 (out of 50), and then see if partial denoising recovers a clean image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab one sample from the dataset\n",
    "sample_img, _ = next(iter(train_loader))\n",
    "sample_img = sample_img[0:1].to(device)  # take the first image in the batch\n",
    "\n",
    "# We'll pretend we are at step t=25 (the midpoint)\n",
    "t_start = 25\n",
    "\n",
    "# Use the library's internal method to generate x_t from x_0\n",
    "with torch.no_grad():\n",
    "    noisy_img = diffusion.q_sample(sample_img, t=torch.tensor([t_start], device=device))\n",
    "\n",
    "print('Original clean image:')\n",
    "show_tensor_image(sample_img[0], title=\"Clean (x_0)\")\n",
    "print('Noisy image at step t=25:')\n",
    "show_tensor_image(noisy_img[0], title=f\"x_{t_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03452d",
   "metadata": {},
   "source": [
    "### 4.2 Denoise from Step 25 → 0\n",
    "Now we apply our `partial_denoise` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a654e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    denoised_img = partial_denoise(diffusion, noisy_img, t_start=t_start)\n",
    "\n",
    "print('Denoised image (x_0 predicted):')\n",
    "show_tensor_image(denoised_img[0], title=\"Denoised x_0_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecbf6c",
   "metadata": {},
   "source": [
    "You’ll likely see that the denoised image is still quite *blobby*, because we trained for only a few steps on random data. With a real dataset and longer training, you’ll see a much clearer recovery of details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659b517",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Replace `FakeData` with a *real* image folder or custom dataset.\n",
    "- Increase `timesteps` (e.g. 1000) for better results.\n",
    "- Train for many more iterations (tens or hundreds of thousands) until convergence.\n",
    "- Adjust the U-Net architecture (bigger `dim`, more `dim_mults`) for higher-quality denoising.\n",
    "- Use advanced sampling methods (like DDIM or DPM-Solver) for faster inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee5d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34e8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "name": "DDPM_Denoising_Example"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
